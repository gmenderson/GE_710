{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_dict = load_from_disk('/home/grenders95/710/710_project/data/training/hf_datasets/newreqs_clearedfields')\n",
    "\n",
    "test_dataset_nr = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['org_repo_name', 'requirements', 'sbom', 'mod_manifest_newreqs', 'num_dependencies', 'sbom_data_cleared', '__index_level_0__'],\n",
      "    num_rows: 532\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946eb19374fc430c86153489fde1b176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  \n",
    "    quantization_config=bnb_config,  \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/home/grenders95/model_ckpts/llama2_newreqscutcont_329/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = eval_tokenizer(prompt)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama2 Evaluation Prompt:\n",
      " <s>[INST] Given a target manifest file, construct the corresponding SPDX SBOM in .json format.\n",
      "\n",
      "### Target manifest:\n",
      "Org: huawei-noah\n",
      "Repo: smarts\n",
      "downloadLocation: git+https://github.com/huawei-noah/SMARTS\n",
      "licenseDeclared: MIT\n",
      "Dependancies:\n",
      "absl-py==2.1.0\n",
      "aiosignal==1.3.1\n",
      "astunparse==1.6.3\n",
      "attrs==23.2.0\n",
      "Automat==22.10.0\n",
      "cachetools==5.3.2\n",
      "certifi==2024.2.2\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "cloudpickle==3.0.0\n",
      "constantly==23.10.4\n",
      "coverage==7.4.1\n",
      "decorator==5.1.1\n",
      "dm-tree==0.1.8\n",
      "exceptiongroup==1.2.0\n",
      "execnet==2.0.2\n",
      "filelock==3.13.1\n",
      "flatbuffers==23.5.26\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.2.0\n",
      "future==0.18.3\n",
      "gast==0.4.0\n",
      "google-auth==2.27.0\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.60.1\n",
      "h5py==3.10.0\n",
      "hyperlink==21.0.0\n",
      "idna==3.6\n",
      "importlib-metadata==7.0.1\n",
      "importlib-resources==6.1.1\n",
      "incremental==22.10.0\n",
      "iniconfig==2.0.0\n",
      "Jinja2==3.1.3\n",
      "jsonschema==4.21.1\n",
      "jsonschema-specifications==2023.12.1\n",
      "keras==2.13.1\n",
      "libclang==16.0.6\n",
      "Markdown==3.5.2\n",
      "MarkupSafe==2.1.5\n",
      "mpmath==1.3.0\n",
      "msgpack==1.0.7\n",
      "networkx==3.1\n",
      "numpy==1.24.3\n",
      "nvidia-cublas-cu12==12.1.3.1\n",
      "nvidia-cuda-cupti-cu12==12.1.105\n",
      "nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "nvidia-cuda-runtime-cu12==12.1.105\n",
      "nvidia-cudnn-cu12==8.9.2.26\n",
      "nvidia-cufft-cu12==11.0.2.54\n",
      "nvidia-curand-cu12==10.3.2.106\n",
      "nvidia-cusolver-cu12==11.4.5.107\n",
      "nvidia-cusparse-cu12==12.1.0.106\n",
      "nvidia-nccl-cu12==2.18.1\n",
      "nvidia-nvjitlink-cu12==12.3.101\n",
      "nvidia-nvtx-cu12==12.1.105\n",
      "oauthlib==3.2.2\n",
      "opencv-python==4.9.0.80\n",
      "opencv-python-headless==4.9.0.80\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.2\n",
      "Panda3D==1.10.14\n",
      "panda3d-gltf==0.13\n",
      "panda3d-simplepbr==0.10\n",
      "pillow==10.2.0\n",
      "pkgutil_resolve_name==1.3.10\n",
      "pluggy==1.4.0\n",
      "protobuf==4.25.2\n",
      "psutil==5.9.8\n",
      "py==1.11.0\n",
      "py-cpuinfo==9.0.0\n",
      "pyasn1==0.5.1\n",
      "pyasn1-modules==0.3.0\n",
      "pybullet==3.2.6\n",
      "pytest==8.0.0\n",
      "pytest-benchmark==4.0.0\n",
      "pytest-cov==4.1.0\n",
      "pytest-forked==1.6.0\n",
      "pytest-xdist==3.5.0\n",
      "PyYAML==6.0.1\n",
      "ray==2.9.0\n",
      "referencing==0.33.0\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rpds-py==0.17.1\n",
      "rsa==4.9\n",
      "scipy==1.10.1\n",
      "shapely==2.0.2\n",
      "six==1.16.0\n",
      "sympy==1.12\n",
      "tableprint==0.9.1\n",
      "tensorboard==2.13.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.13.1\n",
      "tensorflow-estimator==2.13.0\n",
      "tensorflow-io-gcs-filesystem==0.34.0\n",
      "tensorflow-probability==0.21.0\n",
      "termcolor==2.4.0\n",
      "tomli==2.0.1\n",
      "torch==2.1.2\n",
      "torchvision==0.16.2\n",
      "trimesh==4.1.3\n",
      "triton==2.1.0\n",
      "Twisted==23.10.0\n",
      "typing_extensions==4.5.0\n",
      "urllib3==2.2.0\n",
      "wcwidth==0.2.13\n",
      "Werkzeug==3.0.1\n",
      "wrapt==1.16.0\n",
      "yattag==1.15.2\n",
      "zipp==3.17.0\n",
      "zope.interface==6.1\n",
      "\n",
      "\n",
      "### SPDX SBOM:\n",
      " [/INST]</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_llama2_evaluation_prompt(data_point):\n",
    "\n",
    "    instruction = f\"Given a target manifest file, construct the corresponding SPDX SBOM in .json format.\\n\\n### Target manifest:\\n{data_point['mod_manifest_newreqs']}\\n\\n### SPDX SBOM:\\n\"\n",
    "    \n",
    "    llama2_instruction = f\"<s>[INST] {instruction} [/INST]</s>\"\n",
    "\n",
    "    full_prompt = f\"{llama2_instruction}\\n\"\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(dataset, batch_size=8):\n",
    "    # Convert the dataset to a list of dictionaries (if not already in this format)\n",
    "    dataset_list = [item for item in dataset]\n",
    "    batches = [dataset_list[i:i + batch_size] for i in range(0, len(dataset_list), batch_size)]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_for_batch(batch, tokenizer, model):\n",
    "    prompts = [generate_llama2_evaluation_prompt(item) for item in batch]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=500).to(\"cuda\")\n",
    "    \n",
    "    # Using autocast for mixed precision to enhance performance\n",
    "    with torch.no_grad(), autocast():\n",
    "        generated_tokens = model.generate(**inputs, max_new_tokens=1380, use_cache=True)\n",
    "    \n",
    "    generated_texts = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in generated_tokens]\n",
    "    return generated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 3  \n",
    "batches = create_batches(test_dataset_nr, batch_size)\n",
    "all_generated_sboms = []\n",
    "\n",
    "for batch in tqdm(batches, desc=\"Generating SBOMs\"):\n",
    "    generated_sboms = generate_for_batch(batch, eval_tokenizer, ft_model)\n",
    "    all_generated_sboms.extend(generated_sboms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not isinstance(test_dataset_nr, pd.DataFrame):\n",
    "    test_df = test_dataset_nr.to_pandas()\n",
    "else:\n",
    "    test_df = test_dataset_nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['generated_sbom'] = all_generated_sboms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/home/grenders95/710/710_project/data/excel/llama2_no_relationships_withGT/llama2_inference_april4_df.csv'\n",
    "\n",
    "test_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Updated results saved to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sme_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
