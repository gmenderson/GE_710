# AI-Assisted Generation of Software Bills of Materials for Python Projects

This is a dedicated online replication package for the semester project "AI-Assisted Generation of Software Bills of Materials for Python Projects". Below, we will provide a detailed guide for recreating results found in this paper. This README will provide a general explaination of the file/repository structure and directions on how to use the tool. Note that more specific instructions can be found within markdown cells and comments within the notebooks. 

## Python Environment

In order to execute notebooks within this repository, you will need to set up a dedicated environment using conda or venv. Once the initial environment is created with Python 3.11.5, you can run the following command in the root directory of the project to correctly set up the environment.  
```
pip install -r requirements.txt
```

Make sure the environment is activated before attempting to run code within the notebooks.

## dataset_creator.ipynb

This Jupyter Notebook was used in mining the relevant data used in this project. To function correctly, you need to add your GitHub username and access token to the specific lines at the top of the file. After that step, you can add mine specific GitHub organizations for requirments.txt and SPDX .json SBOM files by executing the single cell. Organizations for mining can be added or removed from the relevant line near the end of the file. The script was designed to use ThreadPoolExecutor to run parallel mining processes for multiple organizations, but we found that mining one organzation at a time lead to fewer time-out errors. The files associated with an repository will be located in a directory following the naming convention "org_reponame", under data/python_spdx. 

## codet5small_eval.ipynb

This Jupyter Notebook is the core of the project. The first part of the notebook is used to create the preprocessed HuggingFace SBOM dataset used in fine-tuning and evaluating the CodeT5-small model. Next the model is fine-tuned on the training dataset and evaluated on the task of generating SPDX SBOM IRs from requirements.txt files in terms of accuracy, Levenshtein distance, and cosine similarity. The evaluation results for each repository in the test set are saved to /data/testset_eval.xlxs for viewing. Token lengths are calculated for the the preprocessed requirements.txt and SPDX SBOM for inspection and repository selection for the ChatGPT evaluation. These results are saved in /data/token_length_eval.xlsx

## chatgpt_eval.ipynb

This Juypter Notebook performs evaluation of the ChatGPT generated SBOMs against the ground truth SBOMs. For this notebook to work as intended, the correct repository directories for evaluation must exist within /data/gpt_eval. The contents of each repository directory should be the requirements.txt file, the ground truth SPDX .json SBOM file, and the SPDX .json SBOM generated by ChatGPT. Once the correct files are in place, this notebook will read the information into a pandas dataframe and evaluate the ground truth SBOMs against the ChatGPT SBOMs in terms of Levenshtein distance and cosine similarity. The results are then saved to /data/gpt_eval.xlsx for inspection. 

## Data directory

In this directory, all the relevant data files for the project are stored. /python_spdx contains all repository directories, which contain a requirements.txt and a GitHub generated SPDX .json SBOM for the corresponding repository. /gpt_eval contains repository directories targeted for ChatGPT evaluation, and each repository directory should contain the requirements.txt file, the ground truth SPDX .json SBOM file, and the SPDX .json SBOM generated by ChatGPT. The .xlsx files within /data contain relevant result information for various stages of the evaluation. 
